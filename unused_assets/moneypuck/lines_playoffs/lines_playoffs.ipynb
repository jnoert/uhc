{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url\n",
      "0  https://moneypuck.com/moneypuck/playerData/sea...\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2009/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2010/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2011/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2012/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2013/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2014/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2015/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2016/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2017/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2018/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2019/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2020/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2021/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2022/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2023/playoffs/lines.csv\n",
      "https://moneypuck.com/moneypuck/playerData/seasonSummary/2024/playoffs/lines.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "file_path = 'unused_assets/moneypuck/lines_playoffs/url.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print the DataFrame to check its content\n",
    "print(df)\n",
    "\n",
    "# Step 2: Extract the URL from the first row, first column (assuming the URL is in the first row)\n",
    "url = df.iloc[0, 0]  # Use iloc[0, 0] for the first row, first column\n",
    "\n",
    "# Step 3: Build a list of URLs from 2009 to 2024\n",
    "base_url = url.replace('/2008/', '/{year}/')  # Replace the year 2008 with a placeholder\n",
    "\n",
    "urls = [base_url.format(year=year) for year in range(2009, 2025)]  # Generate URLs for years 2009 to 2024\n",
    "\n",
    "# Step 4: Print the generated URLs\n",
    "for url in urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/Documents/git/NHL/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded unused_assets/moneypuck/lines_playoffs/0_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/1_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/2_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/3_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/4_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/5_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/6_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/7_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/8_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/9_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/10_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/11_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/12_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/13_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/14_lines.csv\n",
      "Downloaded unused_assets/moneypuck/lines_playoffs/15_lines.csv\n",
      "Failed to download https://moneypuck.com/moneypuck/playerData/seasonSummary/2024/playoffs/lines.csv\n",
      "All downloaded files have been aggregated into 'unused_assets/moneypuck/lines_playoffs/lines_playoffs_agg.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "file_path = 'unused_assets/moneypuck/lines_playoffs/url.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Define the directory to save the files\n",
    "save_dir = 'unused_assets/moneypuck/lines_playoffs/'\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize an empty DataFrame for aggregation\n",
    "aggregated_df = pd.DataFrame()\n",
    "\n",
    "# Step 3: Loop through each URL in the 'url' column and download the file with a unique name\n",
    "for index, row in df.iterrows():\n",
    "    url = row['url']  # Using the 'url' column\n",
    "    parsed_url = urlparse(url)\n",
    "    base_name = os.path.basename(parsed_url.path)\n",
    "    unique_name = f\"{index}_{base_name}\"  # Add the index to ensure a unique name\n",
    "    file_name = os.path.join(save_dir, unique_name)\n",
    "\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded {file_name}\")\n",
    "\n",
    "        # Step 4: Read the downloaded CSV and append it to the aggregated DataFrame\n",
    "        temp_df = pd.read_csv(file_name)\n",
    "        aggregated_df = pd.concat([aggregated_df, temp_df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Failed to download {url}\")\n",
    "\n",
    "# Step 5: Save the aggregated DataFrame to a new CSV file\n",
    "aggregated_output_file = os.path.join(save_dir, 'lines_playoffs_agg.csv')\n",
    "aggregated_df.to_csv(aggregated_output_file, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"All downloaded files have been aggregated into '{aggregated_output_file}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
