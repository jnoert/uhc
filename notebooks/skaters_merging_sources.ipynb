{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORKING DOC for merging player-level data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where 'position' = 'G' have been saved to assets/quanthockey/quanthockey_playoff_merged_g.csv.\n",
      "Rows where 'position' != 'G' have been saved to assets/quanthockey/quanthockey_playoff_merged_clean.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "asset = 'assets/quanthockey/quanthockey_playoff_merged.csv'\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(asset)\n",
    "\n",
    "# Extract all rows where 'position' = 'G'\n",
    "df_position_g = df[df['position'] == 'G']\n",
    "\n",
    "# Extract all rows where 'position' != 'G'\n",
    "df_position_not_g = df[df['position'] != 'G']\n",
    "\n",
    "# Save the filtered data to new CSV files\n",
    "output_g = 'assets/quanthockey/quanthockey_playoff_merged_g.csv'\n",
    "df_position_g.to_csv(output_g, index=False)\n",
    "\n",
    "output_not_g = 'assets/quanthockey/quanthockey_playoff_merged_clean.csv'\n",
    "df_position_not_g.to_csv(output_not_g, index=False)\n",
    "\n",
    "print(f\"Rows where 'position' = 'G' have been saved to {output_g}.\")\n",
    "print(f\"Rows where 'position' != 'G' have been saved to {output_not_g}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where 'position' = 'G' have been saved to assets/quanthockey/quanthockey_reg_merged_g.csv.\n",
      "Rows where 'position' != 'G' have been saved to assets/quanthockey/quanthockey_reg_merged_clean.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "asset = 'assets/quanthockey/quanthockey_reg_merged.csv'\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(asset)\n",
    "\n",
    "# Extract all rows where 'position' = 'G'\n",
    "df_position_g = df[df['position'] == 'G']\n",
    "\n",
    "# Extract all rows where 'position' != 'G'\n",
    "df_position_not_g = df[df['position'] != 'G']\n",
    "\n",
    "# Save the filtered data to new CSV files\n",
    "output_g = 'assets/quanthockey/quanthockey_reg_merged_g.csv'\n",
    "df_position_g.to_csv(output_g, index=False)\n",
    "\n",
    "output_not_g = 'assets/quanthockey/quanthockey_reg_merged_clean.csv'\n",
    "df_position_not_g.to_csv(output_not_g, index=False)\n",
    "\n",
    "print(f\"Rows where 'position' = 'G' have been saved to {output_g}.\")\n",
    "print(f\"Rows where 'position' != 'G' have been saved to {output_not_g}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names from skaters-merged_all.csv not merged:\n",
      "['Aaron Downey' 'Aaron Voros' 'Adam Burish' ... 'Zach Dean' 'Zack Bolduc'\n",
      " 'Zack Ostapchuk']\n",
      "\n",
      "Names from assets/quanthockey/quanthockey_reg_merged_clean.csv not merged:\n",
      "['Aaron Downey' 'Aaron Voros' 'Adam Burish' ... 'Zach Dean' 'Zack Bolduc'\n",
      " 'Zack Ostapchuk']\n",
      "The merged file has been saved as 'assets/skaters_quanthockey_merged.csv'.\n"
     ]
    }
   ],
   "source": [
    "## Program to merge\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV files\n",
    "skaters_df = pd.read_csv('assets/skaters-merged_all.csv')\n",
    "quanthockey_df = pd.read_csv('assets/quanthockey/quanthockey_reg_merged_clean.csv')\n",
    "\n",
    "# # Step 2: Standardize the 'name' column to lowercase in both DataFrames\n",
    "# skaters_df['name'] = skaters_df['name'].str.lower()\n",
    "# quanthockey_df['name'] = quanthockey_df['name'].str.lower()\n",
    "\n",
    "# Step 3: Merge the DataFrames on 'season', 'name', and 'position'\n",
    "merged_df = pd.merge(skaters_df, quanthockey_df, on=['season', 'name','position'], how='outer', indicator=True)\n",
    "\n",
    "# Step 4: Identify and print the names that were not merged\n",
    "not_merged_skaters = merged_df[merged_df['_merge'] == 'left_only']['name'].unique()\n",
    "not_merged_quanthockey = merged_df[merged_df['_merge'] == 'right_only']['name'].unique()\n",
    "\n",
    "print(\"Names from skaters-merged_all.csv not merged:\")\n",
    "print(not_merged_skaters)\n",
    "\n",
    "print(\"\\nNames from assets/quanthockey/quanthockey_reg_merged_clean.csv not merged:\")\n",
    "print(not_merged_quanthockey)\n",
    "\n",
    "# Step 5: Save the merged DataFrame to a new CSV file\n",
    "output_file = 'assets/skaters_quanthockey_merged.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"The merged file has been saved as '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m df_unique \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Step 3: Query column 204 and filter rows where the result matches 'left_only' or 'right_only'\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m column_204 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_unique\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m203\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Use iloc to select column 204 (indexing starts from 0, so 203 is the 204th column)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m df_unique[(column_204 \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft_only\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m|\u001b[39m (column_204 \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright_only\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Step 4: Save the filtered DataFrame to a new CSV file, including 'name' and 'column 204'\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/git/NHL/myenv/lib/python3.9/site-packages/pandas/core/indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/git/NHL/myenv/lib/python3.9/site-packages/pandas/core/indexing.py:1690\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, tup: \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m-> 1690\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_tuple_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1691\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1692\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_lowerdim(tup)\n",
      "File \u001b[0;32m~/Documents/git/NHL/myenv/lib/python3.9/site-packages/pandas/core/indexing.py:966\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_tuple_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key):\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    968\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation based indexing can only have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    970\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] types\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    971\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/git/NHL/myenv/lib/python3.9/site-packages/pandas/core/indexing.py:1592\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_integer(key):\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;66;03m# a tuple should already have been caught by this point\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;66;03m# so don't treat a tuple as a valid indexer\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many indexers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/git/NHL/myenv/lib/python3.9/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "## THIS WILL GIVE US THE LIST OF PLAYER NAMES WHICH DID NOT PROPERLY MERGE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "file_path = 'assets/skaters_quanthockey_merged.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Drop duplicates based on the 'name' column to ensure uniqueness\n",
    "df_unique = df.drop_duplicates(subset='name')\n",
    "\n",
    "# Step 3: Query column 204 and filter rows where the result matches 'left_only' or 'right_only'\n",
    "column_204 = df_unique.iloc[:, 203]  # Use iloc to select column 204 (indexing starts from 0, so 203 is the 204th column)\n",
    "filtered_df = df_unique[(column_204 == 'left_only') | (column_204 == 'right_only')]\n",
    "\n",
    "# Step 4: Save the filtered DataFrame to a new CSV file, including 'name' and 'column 204'\n",
    "output_file = 'assets/filtered_names_left_right_only.csv'\n",
    "filtered_df[['name', column_204.name]].to_csv(output_file, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Filtered results have been saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names from skaters_all.csv not merged (team = 'ARI'):\n",
      "['Alexander Nikulin' 'Brandon Prust' 'Brian McGrattan' 'Daniel Winnik'\n",
      " 'David Hale' 'David Schlemko' 'Dmitri Kalinin' 'Ed Jovanovski'\n",
      " 'Enver Lisin' 'Garth Murray' 'Jeff Hoggan' 'Joakim Lindstrom'\n",
      " 'Joel Perrault' 'Keith Yandle' 'Ken Klee' 'Kevin Porter' 'Kurt Sauer'\n",
      " 'Kyle Turris' 'Martin Hanzal' 'Matthew Lombardi' 'Mikkel Boedker'\n",
      " 'Nigel Dawes' 'Peter Mueller' 'Petr Prucha' 'Scottie Upshall'\n",
      " 'Shane Doan' 'Steve Reinprecht' 'Steven Goertzen' 'Todd Fedoruk'\n",
      " 'Viktor Tikhonov' 'Zbynek Michalek' 'Adrian Aucoin' 'Derek Morris'\n",
      " 'Jim Vandermeer' 'Lauri Korpikoski' 'Lee Stempniak' 'Mathieu Schneider'\n",
      " 'Paul Bissonnette' 'Petteri Nokelainen' 'Radim Vrbata' 'Robert Lang'\n",
      " 'Sami Lepisto' 'Shaun Heshka' 'Taylor Pyatt' 'Vernon Fiddler'\n",
      " 'Wojtek Wolski' 'Andrew Ebbett' 'Brett MacLean' 'Chris Summers'\n",
      " 'Eric Belanger' 'Garrett Stafford' 'Michal Rozsival' 'Nolan Yonkman'\n",
      " 'Oliver Ekman-Larsson' 'Ray Whitney' 'Rostislav Klesla' 'Ryan Hollweg'\n",
      " 'Alexandre Bolduc' 'Andy Miele' 'Antoine Vermette' 'Boyd Gordon'\n",
      " 'David Rundblad' 'Daymond Langkow' 'Gilbert Brule' 'Kyle Chipchura'\n",
      " 'Marc-Antoine Pouliot' 'Matt Watkins' 'Michael Stone'\n",
      " \"Patrick O'Sullivan\" 'Raffi Torres' 'Chris Brown' 'Chris Conner'\n",
      " 'David Moss' 'Nick Johnson' 'Rob Klinkhammer' 'Brandon Gormley'\n",
      " 'Brandon McMillan' 'Brandon Yip' 'Connor Murphy' 'Jeff Halpern'\n",
      " 'Jordan Szwarz' 'Lucas Lessio' 'Martin Erat' 'Mike Ribeiro' 'Tim Kennedy'\n",
      " 'B.J. Crombeen' 'Nicklas Grossmann' 'Tony DeAngelo' 'Vinnie Hinostroza'\n",
      " 'Carl Soderberg' 'Victor Soderstrom' 'Anton Stralman' 'Dmitrij Jaskin'\n",
      " 'Riley Nash' 'Juuso Valimaki' 'Aku Raty' 'Alex Kerfoot']\n",
      "\n",
      "Names from quanthockey_reg_merged.csv not merged (team = 'ARI'):\n",
      "['Brandon Crombeen' 'Louis Domingue' 'Mike McKenna' 'Mike Smith'\n",
      " 'Mikkel Bødker' 'Anders Lindbäck' 'Niklas Grossman' 'Niklas Treutle'\n",
      " 'Zbynek Michálek' 'Anthony DeAngelo' 'Justin Peters' 'Marek Langhamer'\n",
      " 'Adin Hill' 'Antti Raanta' 'Darcy Kuemper' 'Scott Wedgewood'\n",
      " 'Calvin Pickard' 'Hunter Miska' 'Vince Hinostroza' 'Carl Söderberg'\n",
      " 'Ivan Prosvetov' 'Victor Söderström' 'Anton Strålman' 'Dmitri Jaskin'\n",
      " 'Connor Ingram' 'Juuso Välimäki' 'Karel Vejmelka' 'Aku Räty'\n",
      " 'Alexander Kerfoot']\n",
      "The merged file has been saved as 'assets/skaters_quanthockey_merged_ari.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV files\n",
    "skaters_df = pd.read_csv('assets/split_by_situation/skaters_all.csv')\n",
    "quanthockey_df = pd.read_csv('assets/quanthockey/quanthockey_reg_merged.csv')\n",
    "\n",
    "# Step 2: Filter both DataFrames for rows where 'team' is 'ARI'\n",
    "skaters_df_ari = skaters_df[skaters_df['team'] == 'ARI']\n",
    "quanthockey_df_ari = quanthockey_df[quanthockey_df['team'] == 'ARI']\n",
    "\n",
    "# Step 3: Merge the filtered DataFrames on 'season', 'name', and 'position'\n",
    "merged_df = pd.merge(skaters_df_ari, quanthockey_df_ari, on=['season', 'name'], how='outer', indicator=True)\n",
    "\n",
    "# Step 4: Identify and print the names that were not merged\n",
    "not_merged_skaters = merged_df[merged_df['_merge'] == 'left_only']['name'].unique()\n",
    "not_merged_quanthockey = merged_df[merged_df['_merge'] == 'right_only']['name'].unique()\n",
    "\n",
    "print(\"Names from skaters_all.csv not merged (team = 'ARI'):\")\n",
    "print(not_merged_skaters)\n",
    "\n",
    "print(\"\\nNames from quanthockey_reg_merged.csv not merged (team = 'ARI'):\")\n",
    "print(not_merged_quanthockey)\n",
    "\n",
    "# Step 5: Save the merged DataFrame to a new CSV file\n",
    "output_file = 'assets/skaters_quanthockey_merged_ari.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"The merged file has been saved as '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names from skaters_all.csv not merged (team = 'ARI'):\n",
      "['Alexander Nikulin' 'Brandon Prust' 'Brian McGrattan' 'Daniel Winnik'\n",
      " 'David Hale' 'David Schlemko' 'Dmitri Kalinin' 'Ed Jovanovski'\n",
      " 'Enver Lisin' 'Garth Murray' 'Jeff Hoggan' 'Joakim Lindstrom'\n",
      " 'Joel Perrault' 'Keith Yandle' 'Ken Klee' 'Kevin Porter' 'Kurt Sauer'\n",
      " 'Kyle Turris' 'Martin Hanzal' 'Matthew Lombardi' 'Mikkel Boedker'\n",
      " 'Nigel Dawes' 'Peter Mueller' 'Petr Prucha' 'Scottie Upshall'\n",
      " 'Shane Doan' 'Steve Reinprecht' 'Steven Goertzen' 'Todd Fedoruk'\n",
      " 'Viktor Tikhonov' 'Zbynek Michalek' 'Adrian Aucoin' 'Derek Morris'\n",
      " 'Jim Vandermeer' 'Lauri Korpikoski' 'Lee Stempniak' 'Mathieu Schneider'\n",
      " 'Paul Bissonnette' 'Petteri Nokelainen' 'Radim Vrbata' 'Robert Lang'\n",
      " 'Sami Lepisto' 'Shaun Heshka' 'Taylor Pyatt' 'Vernon Fiddler'\n",
      " 'Wojtek Wolski' 'Andrew Ebbett' 'Brett MacLean' 'Chris Summers'\n",
      " 'Eric Belanger' 'Garrett Stafford' 'Michal Rozsival' 'Nolan Yonkman'\n",
      " 'Oliver Ekman-Larsson' 'Ray Whitney' 'Rostislav Klesla' 'Ryan Hollweg'\n",
      " 'Alexandre Bolduc' 'Andy Miele' 'Antoine Vermette' 'Boyd Gordon'\n",
      " 'David Rundblad' 'Daymond Langkow' 'Gilbert Brule' 'Kyle Chipchura'\n",
      " 'Marc-Antoine Pouliot' 'Matt Watkins' 'Michael Stone'\n",
      " \"Patrick O'Sullivan\" 'Raffi Torres' 'Chris Brown' 'Chris Conner'\n",
      " 'David Moss' 'Nick Johnson' 'Rob Klinkhammer' 'Brandon Gormley'\n",
      " 'Brandon McMillan' 'Brandon Yip' 'Connor Murphy' 'Jeff Halpern'\n",
      " 'Jordan Szwarz' 'Lucas Lessio' 'Martin Erat' 'Mike Ribeiro' 'Tim Kennedy'\n",
      " 'B.J. Crombeen' 'Nicklas Grossmann' 'Tony DeAngelo' 'Vinnie Hinostroza'\n",
      " 'Carl Soderberg' 'Victor Soderstrom' 'Anton Stralman' 'Dmitrij Jaskin'\n",
      " 'Riley Nash' 'Juuso Valimaki' 'Aku Raty' 'Alex Kerfoot']\n",
      "\n",
      "Names from quanthockey_reg_merged.csv not merged (team = 'ARI'):\n",
      "['Brandon Crombeen' 'Louis Domingue' 'Mike McKenna' 'Mike Smith'\n",
      " 'Mikkel Bødker' 'Anders Lindbäck' 'Niklas Grossman' 'Niklas Treutle'\n",
      " 'Zbynek Michálek' 'Anthony DeAngelo' 'Justin Peters' 'Marek Langhamer'\n",
      " 'Adin Hill' 'Antti Raanta' 'Darcy Kuemper' 'Scott Wedgewood'\n",
      " 'Calvin Pickard' 'Hunter Miska' 'Vince Hinostroza' 'Carl Söderberg'\n",
      " 'Ivan Prosvetov' 'Victor Söderström' 'Anton Strålman' 'Dmitri Jaskin'\n",
      " 'Connor Ingram' 'Juuso Välimäki' 'Karel Vejmelka' 'Aku Räty'\n",
      " 'Alexander Kerfoot']\n",
      "The merged file has been saved as 'assets/skaters_quanthockey_merged_ari.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV files\n",
    "skaters_df = pd.read_csv('assets/split_by_situation/skaters_all.csv')\n",
    "quanthockey_df = pd.read_csv('assets/quanthockey/quanthockey_reg_merged.csv')\n",
    "\n",
    "# Step 2: Filter both DataFrames for rows where 'team' is 'ARI'\n",
    "skaters_df_ari = skaters_df[skaters_df['team'] == 'ARI']\n",
    "quanthockey_df_ari = quanthockey_df[quanthockey_df['team'] == 'ARI']\n",
    "\n",
    "# Step 3: Merge the filtered DataFrames on 'season', 'name', and 'position'\n",
    "merged_df = pd.merge(skaters_df_ari, quanthockey_df_ari, on=['season', 'name'], how='outer', indicator=True)\n",
    "\n",
    "# # Step 4: Exclude rows where 'position_y' is 'G'\n",
    "# merged_df = merged_df[merged_df['position'] != 'G']\n",
    "\n",
    "# Step 5: Identify and print the names that were not merged\n",
    "not_merged_skaters = merged_df[merged_df['_merge'] == 'left_only']['name'].unique()\n",
    "not_merged_quanthockey = merged_df[merged_df['_merge'] == 'right_only']['name'].unique()\n",
    "\n",
    "print(\"Names from skaters_all.csv not merged (team = 'ARI'):\")\n",
    "print(not_merged_skaters)\n",
    "\n",
    "print(\"\\nNames from quanthockey_reg_merged.csv not merged (team = 'ARI'):\")\n",
    "print(not_merged_quanthockey)\n",
    "\n",
    "# Step 6: Save the merged DataFrame to a new CSV file\n",
    "output_file = 'assets/skaters_quanthockey_merged_ari.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"The merged file has been saved as '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've discovered that the reason so much data from QuantHockey isn't merging is because the abbreviation for ARI used to be PHX.\n",
    "The following code fixes this in BOTH the QuantHockey season and playoff datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PHX' has been replaced with 'ARI' in both files and saved back to 'assets/quanthockey/quanthockey_reg_merged.csv' and 'assets/quanthockey/quanthockey_playoff_merged.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_reg = 'assets/quanthockey/quanthockey_reg_merged.csv'\n",
    "file_playoff = 'assets/quanthockey/quanthockey_playoff_merged.csv'\n",
    "\n",
    "# Step 1: Load the regular season CSV file\n",
    "reg_df = pd.read_csv(file_reg)\n",
    "\n",
    "# Step 2: Replace 'PHX' with 'ARI' in the 'team' column\n",
    "reg_df['team'] = reg_df['team'].replace('PHX', 'ARI')\n",
    "\n",
    "# Step 3: Save the modified DataFrame back to the original file\n",
    "reg_df.to_csv(file_reg, index=False)\n",
    "\n",
    "# Step 4: Load the playoff CSV file\n",
    "playoff_df = pd.read_csv(file_playoff)\n",
    "\n",
    "# Step 5: Replace 'PHX' with 'ARI' in the 'team' column\n",
    "playoff_df['team'] = playoff_df['team'].replace('PHX', 'ARI')\n",
    "\n",
    "# Step 6: Save the modified DataFrame back to the original file\n",
    "playoff_df.to_csv(file_playoff, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"'PHX' has been replaced with 'ARI' in both files and saved back to '{file_reg}' and '{file_playoff}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names from skaters_all.csv not merged (team = 'ARI'):\n",
      "['Joakim Lindstrom' 'Mikkel Boedker' 'Steve Reinprecht' 'Zbynek Michalek'\n",
      " 'Sami Lepisto' 'Vernon Fiddler' 'Michal Rozsival' 'B.J. Crombeen'\n",
      " 'Nicklas Grossmann' 'Tony DeAngelo' 'Vinnie Hinostroza' 'Carl Soderberg'\n",
      " 'Victor Soderstrom' 'Anton Stralman' 'Dmitrij Jaskin' 'Riley Nash'\n",
      " 'Juuso Valimaki' 'Aku Raty' 'Alex Kerfoot']\n",
      "\n",
      "Names from quanthockey_reg_merged.csv not merged (team = 'ARI'):\n",
      "['Al Montoya' 'Ilya Bryzgalov' 'Joakim Lindström' 'Josh Tordjman'\n",
      " 'Mikkel Bødker' 'Steven Reinprecht' 'Zbynek Michálek' 'Jason LaBarbera'\n",
      " 'Sami Lepistö' 'Vern Fiddler' 'Matt Climie' 'Michal Rozsíval'\n",
      " 'Curtis McElhinney' 'Mike Smith' 'Chad Johnson' 'Mark Visentin'\n",
      " 'Thomas Greiss' 'Brandon Crombeen' 'Louis Domingue' 'Mike McKenna'\n",
      " 'Anders Lindbäck' 'Niklas Grossman' 'Niklas Treutle' 'Anthony DeAngelo'\n",
      " 'Justin Peters' 'Marek Langhamer' 'Adin Hill' 'Antti Raanta'\n",
      " 'Darcy Kuemper' 'Scott Wedgewood' 'Calvin Pickard' 'Hunter Miska'\n",
      " 'Vince Hinostroza' 'Carl Söderberg' 'Ivan Prosvetov' 'Victor Söderström'\n",
      " 'Anton Strålman' 'Dmitri Jaskin' 'Connor Ingram' 'Juuso Välimäki'\n",
      " 'Karel Vejmelka' 'Aku Räty' 'Alexander Kerfoot']\n",
      "The merged file has been saved as 'assets/skaters_quanthockey_merged_ari.csv'.\n"
     ]
    }
   ],
   "source": [
    "## same code as two blocks above, but should work better now that the abbreviation issue is fixed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV files\n",
    "skaters_df = pd.read_csv('assets/split_by_situation/skaters_all.csv')\n",
    "quanthockey_df = pd.read_csv('assets/quanthockey/quanthockey_reg_merged.csv')\n",
    "\n",
    "# Step 2: Filter both DataFrames for rows where 'team' is 'ARI'\n",
    "skaters_df_ari = skaters_df[skaters_df['team'] == 'ARI']\n",
    "quanthockey_df_ari = quanthockey_df[quanthockey_df['team'] == 'ARI']\n",
    "\n",
    "# Step 3: Merge the filtered DataFrames on 'season', 'name', and 'position'\n",
    "merged_df = pd.merge(skaters_df_ari, quanthockey_df_ari, on=['season', 'name'], how='outer', indicator=True)\n",
    "\n",
    "# # Step 4: Exclude rows where 'position_y' is 'G'\n",
    "# merged_df = merged_df[merged_df['position'] != 'G']\n",
    "\n",
    "# Step 5: Identify and print the names that were not merged\n",
    "not_merged_skaters = merged_df[merged_df['_merge'] == 'left_only']['name'].unique()\n",
    "not_merged_quanthockey = merged_df[merged_df['_merge'] == 'right_only']['name'].unique()\n",
    "\n",
    "print(\"Names from skaters_all.csv not merged (team = 'ARI'):\")\n",
    "print(not_merged_skaters)\n",
    "\n",
    "print(\"\\nNames from quanthockey_reg_merged.csv not merged (team = 'ARI'):\")\n",
    "print(not_merged_quanthockey)\n",
    "\n",
    "# Step 6: Save the merged DataFrame to a new CSV file\n",
    "output_file = 'assets/skaters_quanthockey_merged_ari.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"The merged file has been saved as '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to 'assets/quanthockey/multiple_teams_per_name.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "file_path = 'assets/skaters-merged_all.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Group by 'season' and 'name', and filter for those with more than one 'team'\n",
    "multiple_teams = df.groupby(['season', 'name']).filter(lambda x: x['team'].nunique() > 1)\n",
    "\n",
    "# Step 3: Save the results to a new CSV file\n",
    "output_file = 'assets/quanthockey/multiple_teams_per_name.csv'\n",
    "multiple_teams.to_csv(output_file, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Results have been saved to '{output_file}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
